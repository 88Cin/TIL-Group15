{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d219c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f142267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_path(folder_path,file_name):\n",
    "    full_path = os.path.join(folder_path, file_name)\n",
    "    return full_path\n",
    "dataset_path=\"H:\\\\Desktop\\\\Q1\\\\python\\\\project\\\\TIL-Group15\\\\dataset\\\\CarFollowing\\\\trainHA.zarr\\\\\"\n",
    "folder_path = \"H:\\\\Desktop\\\\Q1\\\\python\\\\project\\\\TIL-Group15\\\\dataset\\\\split_data\\\\HA\\\\\"\n",
    "filename_HA='trainHA.zarr'\n",
    "filename_multi_data='multi_data.csv'\n",
    "filename_multi_size='multi_size.csv'\n",
    "filename_distance='distance.csv'\n",
    "filename_regimeHA='regimes_list_HA_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "964c7a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to export the center point, velocity and acceleration of the vehicles in dataset\n",
    "#Define the number of exports with inputs\n",
    "def load_single_data(i):    \n",
    "    data = zarr.open(dataset_path, mode='a')\n",
    "    start, end = data.index_range[i]\n",
    "    # get timestamps\n",
    "    timestamps = data.timestamp[start:end]\n",
    "    # get position, speed, and acceleration\n",
    "    x_lead = data.lead_centroid[start:end]\n",
    "    v_lead = data.lead_velocity[start:end]\n",
    "    a_lead = data.lead_acceleration[start:end]\n",
    "    case_id=np.ones((1,x_lead.shape[0]))*i\n",
    "    x_follow = data.follow_centroid[start:end]\n",
    "    v_follow = data.follow_velocity[start:end]\n",
    "    a_follow = data.follow_acceleration[start:end]\n",
    "    array = np.vstack((case_id, x_lead, v_lead, a_lead, x_follow, v_follow, a_follow, timestamps))\n",
    "    return array.T\n",
    "#Define a function to export the size of the vehicles in dataset\n",
    "def load_size(i):\n",
    "    data = zarr.open(dataset_path, mode='a')\n",
    "    size_lead = 4.85 # this is the size of the lead vehiclesï¼Œfor HA there's only one size\n",
    "    size_follow = data.follow_size[i] # this is the size of the follow vehicles\n",
    "    case_id=np.ones(1)*i # ID\n",
    "    array = np.vstack((case_id, size_lead, size_follow)) \n",
    "    return array.T\n",
    "#Organize all data into a csv file\n",
    "def load_multi_size(i):\n",
    "    multi_data = None\n",
    "    for x in range(i + 1):\n",
    "        single_data1 = load_size(x)\n",
    "        if x == 0:\n",
    "            multi_data = single_data1\n",
    "        else:\n",
    "            multi_data = np.vstack([multi_data, single_data1])\n",
    "    return multi_data\n",
    "def load_multi_data(i):\n",
    "    multi_data = None\n",
    "    for x in range(i+1):\n",
    "        single_data = load_single_data(x)\n",
    "        if x == 0:\n",
    "            multi_data = single_data\n",
    "        else:\n",
    "            multi_data = np.vstack([multi_data, single_data])\n",
    "    return multi_data\n",
    "#Define the number of exports with inputs\n",
    "p=int(input('type in the number you want'))\n",
    "\n",
    "np.savetxt(full_path(folder_path,filename_multi_data), load_multi_data(p), delimiter=\",\" ,\n",
    "           header=\"ID,x_lead,v_lead,a_lead,x_follow,v_follow,a_follow,timestamps\",comments='')\n",
    "np.savetxt(full_path(folder_path,filename_multi_size), load_multi_size(p), delimiter=\",\" ,\n",
    "           header=\"ID,size_lead,size_follow\",comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a6b0c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.loadtxt(full_path(folder_path,filename_multi_data),delimiter=\",\",skiprows=1 )\n",
    "data_size=np.loadtxt(full_path(folder_path,filename_multi_size),delimiter=\",\",skiprows=1 )\n",
    "#data_size.columns = [\"ID\", \"size_lead\", \"size_follow\"]\n",
    "#data.columns = [\"ID\",\"x_lead\", \"v_lead\", \"a_lead\", \"x_follow\", \"v_follow\", \"a_follow\", \"timestamps\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "95961011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define A_distance as the vehicle distance (tail of lead vehicle to head of follow vehicle)\n",
    "# Define B_distance is the vehicle distance (head of lead vehicle to head of follow vehicle)\n",
    "\n",
    "def distance(x):\n",
    "    x_data=np.squeeze(data[np.where(data[:,0]==x),:],0)\n",
    "    x_size=np.squeeze(data_size[np.where(data_size[:,0]==x),:],0)\n",
    "    x_lead=x_data[:,1]\n",
    "    x_follow=x_data[:,4]\n",
    "    size_lead = x_size[:,1]\n",
    "    size_follow = x_size[:,2]\n",
    "    id=np.ones(1)*x\n",
    "    A_distance=[]\n",
    "    B_distance=[]\n",
    "    for y in range(1,x_lead.size):\n",
    "        A = x_lead[y] - x_follow[y] - 0.5 * (size_lead[0] + size_follow[0])# A_distance\n",
    "        B = x_lead[y] - x_follow[y] - 0.5 * (size_follow[0] - size_lead[0])# A_distance\n",
    "        A_distance.append(A)\n",
    "        B_distance.append(B)\n",
    "    id_tiled = np.tile(id, len(A_distance))\n",
    "    array = np.vstack((id_tiled, A_distance, B_distance))\n",
    "    return array\n",
    "def load_multi_distance(i):    \n",
    "    multi_data = None\n",
    "    for x in range(i+1):\n",
    "        single_data = distance(x).T\n",
    "        if x == 0:\n",
    "            multi_data = single_data\n",
    "        else:\n",
    "            multi_data = np.vstack([multi_data, single_data])\n",
    "    return multi_data\n",
    "np.savetxt(full_path(folder_path,filename_distance), load_multi_distance(p), delimiter=\",\" ,\n",
    "           header=\"ID,A_distance,B_distance\",comments='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a32b344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_9716\\785762357.py:1: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  data3 = np.genfromtxt(full_path(folder_path,filename_regimeHA), delimiter=',',  dtype=None, names=True)\n"
     ]
    }
   ],
   "source": [
    "data3 = np.genfromtxt(full_path(folder_path,filename_regimeHA), delimiter=',',  dtype=None, names=True)\n",
    "\n",
    "# define the list of regimes\n",
    "pattern = re.compile(r'Fa|Fd|A|D|F|C|S')\n",
    "output_list = []  # the list to store the data\n",
    "for row in data3:\n",
    "    regime = row['regime_comb'].decode('utf-8')\n",
    "    matches = pattern.findall(regime)  # use re to cut regime_comb\n",
    "    row_dict = {}  \n",
    "    for module in matches:\n",
    "        row_dict[module] = row[module]\n",
    "    \n",
    "    # export the data into the list\n",
    "    output_list.append(row_dict)\n",
    "data = np.loadtxt(full_path(folder_path,filename_multi_data), delimiter=\",\",skiprows=1)\n",
    "\n",
    "patterns = [\"A\", \"D\", \"F\", \"Fa\", \"Fd\", \"C\", \"S\"]  # the list of regimes\n",
    "\n",
    "# make a CSV file for each regime\n",
    "for pattern_name in patterns:\n",
    "    with open(full_path(folder_path,f'{pattern_name}.csv'), 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        # write in the title for each line\n",
    "        writer.writerow(['ID', 'x_lead', 'v_lead', 'a_lead', 'x_follow', 'v_follow', 'a_follow', 'timestamps'])\n",
    "\n",
    "        for i in range(len(output_list)):\n",
    "            pattern_times = output_list[i]\n",
    "            all_data = data[data[:, 0] == i, 1:]  # load the data except ID\n",
    "            pattern_data = {}\n",
    "            start_index = 0\n",
    "            for pattern, time in pattern_times.items():\n",
    "                if pattern == pattern_name: \n",
    "                    length = int(time * len(all_data) / sum(pattern_times.values()))\n",
    "                    pattern_data[pattern] = all_data[start_index:start_index+length]\n",
    "                    start_index += length\n",
    "\n",
    "            # save the result in CSV file\n",
    "            for _, pattern_specific_data in pattern_data.items():\n",
    "                for row_data in pattern_specific_data:\n",
    "                    writer.writerow([i] + list(row_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb5017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = np.loadtxt(full_path(folder_path, filename_regimeHA), delimiter=\",\", skiprows=1, dtype=object)\n",
    "multi_data = np.loadtxt(full_path(folder_path, filename_multi_data), delimiter=\",\", skiprows=1, dtype=object)\n",
    "\n",
    "def count_upper_except_adf(regime_str):\n",
    "    # replace 'ADF' to 'A'\n",
    "    if 'ADF' in regime_str:\n",
    "        new_str = re.sub('ADF', 'A', regime_str)\n",
    "    # find all the upper letters\n",
    "        upper_chars = re.findall(r'[A-Z]', new_str)\n",
    "    # return the number of upper letters\n",
    "    else:\n",
    "        upper_chars=[]\n",
    "    return len(upper_chars)\n",
    "\n",
    "# categories\n",
    "categories = {\n",
    "    \"ADF\": [],\n",
    "    \"ADF+1\": [],\n",
    "    \"ADF+2\": [],\n",
    "    \"ADF+3\": [],\n",
    "    \"ADF+4\": []\n",
    "}\n",
    "\n",
    "for row in data:\n",
    "    regime_comb = row[6]  #HH:5 HA:6\n",
    "    regime_id = float(row[0])  # Assuming the ID is in the first column and is a float\n",
    "    if 'ADF' in regime_comb:\n",
    "        regime_count = int(count_upper_except_adf(regime_comb))-1\n",
    "        key = f'ADF+{regime_count}' if regime_count else 'ADF'\n",
    "        categories[key].append(regime_id)\n",
    "\n",
    "# get the data from multi_data, and save in multi csv file.\n",
    "for key, ids in categories.items():\n",
    "    filtered_data = [row for row in multi_data if float(row[0]) in ids]\n",
    "    output_path = full_path(folder_path, f'{key}_multi_data.csv')\n",
    "    np.savetxt(output_path, filtered_data, delimiter=\",\", fmt='%s',header=\"ID,x_lead,v_lead,a_lead,x_follow,v_follow,a_follow,timestamps\", comments='') \n",
    "    # using %s to save as string, adjust if necessary\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('TIL6022')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "235118494f6096765c1deb9d143943fcb560fd7b79d70fb1cbb738bbc852222c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
